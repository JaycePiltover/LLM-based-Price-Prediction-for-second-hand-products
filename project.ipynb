{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30fpZDGFtSA2"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 0. Setup\n",
        "# ============================================================\n",
        "!pip install -q pandas numpy sentence-transformers faiss-cpu scikit-learn matplotlib transformers accelerate peft bitsandbytes\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_log_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# For reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# ============================================================\n",
        "# 1. Data loading and preprocessing (candidate pool)\n",
        "# ============================================================\n",
        "\n",
        "DATA_PATH = \"marketing_sample_for_ebay_com-ebay_com_product__20210101_20210331__30k_data.csv\"\n",
        "\n",
        "df_raw = pd.read_csv(\n",
        "    DATA_PATH,\n",
        "    sep=\",\",\n",
        "    engine=\"python\",\n",
        "    on_bad_lines=\"skip\",\n",
        ")\n",
        "\n",
        "print(\"Raw shape:\", df_raw.shape)\n",
        "print(df_raw.head(2))\n",
        "print(df_raw.columns.tolist())\n",
        "\n",
        "df = df_raw.copy()\n",
        "df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def parse_price(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    s = str(x).replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
        "    try:\n",
        "        return float(s)\n",
        "    except ValueError:\n",
        "        return np.nan\n",
        "\n",
        "def safe_str(x):\n",
        "    return \"\" if pd.isna(x) else str(x)\n",
        "\n",
        "def build_text(row):\n",
        "  parts = [\n",
        "      safe_str(row.get(\"Title\", \"\")),\n",
        "      safe_str(row.get(\"Manufacturer\", \"\")),\n",
        "      safe_str(row.get(\"Model Name\", \"\")),\n",
        "      safe_str(row.get(\"Internal Memory\", \"\")),\n",
        "      safe_str(row.get(\"Screen Size\", \"\")),\n",
        "      safe_str(row.get(\"Carrier\", \"\")),\n",
        "      safe_str(row.get(\"Color Category\", \"\")),\n",
        "      safe_str(row.get(\"Specifications\", \"\")),\n",
        "      safe_str(row.get(\"Model Num\", \"\")) ]\n",
        "  return \" \".join([p for p in parts if p])\n",
        "\n",
        "  def clean_text(txt):\n",
        "    txt = str(txt).lower()\n",
        "    txt = re.sub(r\"[^\\w\\s]\", \" \", txt)\n",
        "    txt = re.sub(r\"\\s+\", \" \", txt)\n",
        "    return txt.strip()\n",
        "\n",
        "  def parse_rating(x):\n",
        "    if pd.isna(x):\n",
        "      return np.nan\n",
        "    s = str(x).strip().replace(\"%\", \"\")\n",
        "    try: return float(s)\n",
        "    except ValueError:\n",
        "      return np.nan\n",
        "\n",
        "  # ---------- numeric target ----------\n",
        "  df[\"price_num\"] = df[\"Price\"].apply(parse_price)\n",
        "  df = df[df[\"price_num\"].notnull() & (df[\"price_num\"] > 0)]\n",
        "  print(\"After price filter:\", df.shape)\n",
        "\n",
        "  # ---------- text fields ----------\n",
        "  df[\"raw_text\"] = df.apply(build_text, axis=1)\n",
        "  df[\"text_clean\"] = df[\"raw_text\"].apply(clean_text)\n",
        "  print(df[[\"raw_text\", \"text_clean\"]].head())\n",
        "\n",
        "  # ---------- extra numeric features ----------\n",
        "  df[\"seller_rating_num\"] = df.get(\"Seller Rating\", np.nan).apply(parse_rating)\n",
        "  df[\"seller_num_reviews_num\"] = pd.to_numeric( df.get(\"Seller Num Of Reviews\"), errors=\"coerce\" )\n",
        "  df[\"num_reviews_num\"] = pd.to_numeric( df.get(\"Num Of Reviews\"), errors=\"coerce\" )\n",
        "  df[\"num_ratings_num\"] = pd.to_numeric( df.get(\"Number Of Ratings\"), errors=\"coerce\" )\n",
        "  df[\"num_avg_rating_num\"] = pd.to_numeric( df.get(\"Average Rating\"), errors=\"coerce\" )\n",
        "\n",
        "  # ---------- unique id ----------\n",
        "  if \"Uniq Id\" in df.columns: df[\"uniq_id\"] = df[\"Uniq Id\"].astype(str)\n",
        "  else: df[\"uniq_id\"] = df.index.astype(str)\n",
        "\n",
        "  feature_cols = [ \"uniq_id\", \"price_num\",\"text_clean\",\"seller_rating_num\", \"seller_num_reviews_num\", \"num_reviews_num\", \"num_ratings_num\", \"num_avg_rating_num\" ]\n",
        "\n",
        "\n",
        "df_final = df[feature_cols].reset_index(drop=True)\n",
        "print(\"Final usable data:\", df_final.shape)\n",
        "df_final.head()\n",
        "\n",
        "# ============================================================\n",
        "# 2. Train / test split (candidate pool -> train, held‑out -> test)\n",
        "# ============================================================\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df_final,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "print(\"Train:\", train_df.shape, \"Test:\", test_df.shape)\n",
        "\n",
        "train_df.to_csv(\"ebay_train_clean.csv\", index=False)\n",
        "test_df.to_csv(\"ebay_test_clean.csv\", index=False)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Text embeddings + FAISS index (similar product retrieval)\n",
        "# ============================================================\n",
        "\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "train_texts = train_df[\"text_clean\"].tolist()\n",
        "test_texts  = test_df[\"text_clean\"].tolist()\n",
        "\n",
        "train_embeddings = embed_model.encode(\n",
        "    train_texts,\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        ")\n",
        "test_embeddings = embed_model.encode(\n",
        "    test_texts,\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        ")\n",
        "\n",
        "print(\"Train embeddings:\", train_embeddings.shape)\n",
        "print(\"Test embeddings:\", test_embeddings.shape)\n",
        "\n",
        "dim = train_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(train_embeddings.astype(\"float32\"))\n",
        "print(\"FAISS index size:\", index.ntotal)\n",
        "\n",
        "def retrieve_similar_for_text(query_text, top_k=5):\n",
        "    emb = embed_model.encode(\n",
        "        [query_text],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True\n",
        "    ).astype(\"float32\")\n",
        "    D, I = index.search(emb, top_k)\n",
        "    sims = train_df.iloc[I[0]][[\"uniq_id\", \"text_clean\", \"price_num\"]].reset_index(drop=True)\n",
        "    return sims\n",
        "\n",
        "# ============================================================\n",
        "# 4. Traditional baselines (KNN, Ridge, RF, GBM, MLP)\n",
        "# ============================================================\n",
        "\n",
        "y_train = train_df[\"price_num\"].values\n",
        "y_test  = test_df[\"price_num\"].values\n",
        "\n",
        "def eval_regressor(model_name, model, X, y_true):\n",
        "    y_pred = model.predict(X)\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_true, np.maximum(y_pred, 1e-6)))\n",
        "    mae   = mean_absolute_error(y_true, y_pred)\n",
        "    print(f\"{model_name}: RMSLE={rmsle:.4f}, MAE={mae:.2f}\")\n",
        "    return {\"name\": model_name, \"rmsle\": rmsle, \"mae\": mae, \"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "# --- KNN baseline ---\n",
        "knn = KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    metric=\"euclidean\",\n",
        "    weights=\"distance\",\n",
        ")\n",
        "knn.fit(train_embeddings, y_train)\n",
        "knn_results = eval_regressor(\"KNN\", knn, test_embeddings, y_test)\n",
        "\n",
        "# --- Ridge regression baseline ---\n",
        "ridge = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
        "ridge.fit(train_embeddings, y_train)\n",
        "ridge_results = eval_regressor(\"Ridge\", ridge, test_embeddings, y_test)\n",
        "\n",
        "# --- Random Forest baseline ---\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=5,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "rf.fit(train_embeddings, y_train)\n",
        "rf_results = eval_regressor(\"RandomForest\", rf, test_embeddings, y_test)\n",
        "\n",
        "# --- Gradient Boosting baseline ---\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "gbr.fit(train_embeddings, y_train)\n",
        "gbr_results = eval_regressor(\"GradientBoosting\", gbr, test_embeddings, y_test)\n",
        "\n",
        "# --- Multimodal DNN baseline ---\n",
        "mm_dnn = Pipeline([\n",
        "    (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
        "    (\"mlp\", MLPRegressor(\n",
        "        hidden_layer_sizes=(256, 128),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        learning_rate_init=1e-3,\n",
        "        max_iter=50,\n",
        "        batch_size=256,\n",
        "        random_state=42,\n",
        "        early_stopping=True,\n",
        "        n_iter_no_change=5,\n",
        "    )),\n",
        "])\n",
        "\n",
        "mm_dnn.fit(X_train_mm, y_train)\n",
        "y_pred_mm = mm_dnn.predict(X_test_mm)\n",
        "\n",
        "rmsle_mm = np.sqrt(mean_squared_log_error(y_test, np.maximum(y_pred_mm, 1e-6)))\n",
        "mae_mm = mean_absolute_error(y_test, y_pred_mm)\n",
        "print(f\"Multimodal DNN: RMSLE={rmsle_mm:.4f}, MAE={mae_mm:.2f}\")\n",
        "\n",
        "multimodal_results = {\n",
        "    \"name\": \"Multimodal DNN\",\n",
        "    \"rmsle\": rmsle_mm,\n",
        "    \"mae\": mae_mm,\n",
        "    \"y_true\": y_test,\n",
        "    \"y_pred\": y_pred_mm,\n",
        "}\n",
        "# ============================================================\n",
        "# 5. Qwen setup (LLM pricing baselines)\n",
        "# ============================================================\n",
        "\n",
        "QWEN_MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    QWEN_MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "def build_qwen_prompt_with_retrieval(target_text, similar_df):\n",
        "    lines = []\n",
        "    for i, row in similar_df.iterrows():\n",
        "        lines.append(\n",
        "            f\"{i+1}. Title: {row['text_clean']}\\n   Final price: {row['price_num']:.2f} USD\"\n",
        "        )\n",
        "    context_block = \"\\n\".join(lines)\n",
        "    prompt = (\n",
        "        \"You are an assistant that suggests fair prices for second-hand electronics.\\n\"\n",
        "        \"Given a target product and similar products with their final prices, \"\n",
        "        \"predict a reasonable price for the target product in USD.\\n\"\n",
        "        \"Return ONLY one positive number (no currency symbol, no explanation).\\n\\n\"\n",
        "        f\"Target product:\\n{target_text}\\n\\n\"\n",
        "        \"Similar products:\\n\"\n",
        "        f\"{context_block}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def build_qwen_prompt_no_retrieval(target_text):\n",
        "    prompt = (\n",
        "        \"You are an assistant that suggests fair prices for second-hand electronics.\\n\"\n",
        "        \"Estimate a reasonable market price in USD for the product below.\\n\"\n",
        "        \"Respond with only one positive number (no currency symbol, no explanation).\\n\\n\"\n",
        "        f\"Product description:\\n{target_text}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "def qwen_generate(text, max_new_tokens=16, temperature=0.2):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(qwen_model.device)\n",
        "    with torch.no_grad():\n",
        "        out = qwen_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    gen_ids = out[0, inputs[\"input_ids\"].shape[1]:]\n",
        "    resp = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return resp.strip()\n",
        "\n",
        "price_pattern = re.compile(r\"\\d+(\\.\\d+)?\")\n",
        "\n",
        "def qwen_price_with_confidence(prompt, n_samples=3, temperature=0.4, max_new_tokens=16):\n",
        "    prices, outputs = [], []\n",
        "    for _ in range(n_samples):\n",
        "        text = qwen_generate(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "        outputs.append(text)\n",
        "        m = price_pattern.search(text)\n",
        "        if not m:\n",
        "            continue\n",
        "        p = float(m.group())\n",
        "        # simple sanity clamp; adjust bounds if needed\n",
        "        if p <= 0 or p > 5000:\n",
        "            continue\n",
        "        prices.append(p)\n",
        "    if not prices:\n",
        "        return None, None, outputs\n",
        "    mean_p = float(np.mean(prices))\n",
        "    std_p  = float(np.std(prices))\n",
        "    return mean_p, std_p, outputs\n",
        "\n",
        "# ============================================================\n",
        "# 6. Qwen with retrieval + confidence filtering (main method)\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_qwen_with_retrieval(\n",
        "    test_df,\n",
        "    top_k=5,\n",
        "    n_samples=5,\n",
        "    temperature=0.3,\n",
        "    max_items=100,\n",
        "    std_threshold=30.0,\n",
        "):\n",
        "    y_true_list, y_pred_list, std_list = [], [], []\n",
        "    for idx, row in test_df.head(max_items).iterrows():\n",
        "        target_text = row[\"text_clean\"]\n",
        "        sims = retrieve_similar_for_text(target_text, top_k=top_k)\n",
        "        prompt = build_qwen_prompt_with_retrieval(target_text, sims)\n",
        "        mean_price, std_price, _ = qwen_price_with_confidence(\n",
        "            prompt,\n",
        "            n_samples=n_samples,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        if mean_price is None:\n",
        "            continue\n",
        "        if std_price is not None and std_price > std_threshold:\n",
        "            # low‑confidence -> drop\n",
        "            continue\n",
        "        y_true_list.append(row[\"price_num\"])\n",
        "        y_pred_list.append(mean_price)\n",
        "        std_list.append(std_price)\n",
        "    if not y_true_list:\n",
        "        print(\"No valid Qwen predictions.\")\n",
        "        return None\n",
        "    y_true = np.array(y_true_list)\n",
        "    y_pred = np.array(y_pred_list)\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_true, np.maximum(y_pred, 1e-6)))\n",
        "    mae   = mean_absolute_error(y_true, y_pred)\n",
        "    print(f\"Qwen + retrieval on {len(y_true)} items: RMSLE={rmsle:.4f}, MAE={mae:.2f}\")\n",
        "    return {\n",
        "        \"rmsle\": rmsle,\n",
        "        \"mae\": mae,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"std\": np.array(std_list),\n",
        "    }\n",
        "\n",
        "qwen_ret_results = evaluate_qwen_with_retrieval(\n",
        "    test_df,\n",
        "    top_k=5,\n",
        "    n_samples=5,\n",
        "    temperature=0.3,\n",
        "    max_items=100,\n",
        "    std_threshold=30.0,\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 7. Qwen no‑retrieval baseline\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_qwen_no_retrieval(\n",
        "    test_df,\n",
        "    n_samples=5,\n",
        "    temperature=0.3,\n",
        "    max_items=100,\n",
        "    std_threshold=30.0,\n",
        "):\n",
        "    y_true_list, y_pred_list, std_list = [], [], []\n",
        "    for idx, row in test_df.head(max_items).iterrows():\n",
        "        target_text = row[\"text_clean\"]\n",
        "        prompt = build_qwen_prompt_no_retrieval(target_text)\n",
        "        mean_price, std_price, _ = qwen_price_with_confidence(\n",
        "            prompt,\n",
        "            n_samples=n_samples,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        if mean_price is None:\n",
        "            continue\n",
        "        if std_price is not None and std_price > std_threshold:\n",
        "            continue\n",
        "        y_true_list.append(row[\"price_num\"])\n",
        "        y_pred_list.append(mean_price)\n",
        "        std_list.append(std_price)\n",
        "    if not y_true_list:\n",
        "        print(\"No valid Qwen predictions.\")\n",
        "        return None\n",
        "    y_true = np.array(y_true_list)\n",
        "    y_pred = np.array(y_pred_list)\n",
        "    rmsle = np.sqrt(mean_squared_log_error(y_true, np.maximum(y_pred, 1e-6)))\n",
        "    mae   = mean_absolute_error(y_true, y_pred)\n",
        "    print(f\"Qwen NO retrieval on {len(y_true)} items: RMSLE={rmsle:.4f}, MAE={mae:.2f}\")\n",
        "    return {\n",
        "        \"rmsle\": rmsle,\n",
        "        \"mae\": mae,\n",
        "        \"y_true\": y_true,\n",
        "        \"y_pred\": y_pred,\n",
        "        \"std\": np.array(std_list),\n",
        "    }\n",
        "\n",
        "qwen_no_ret_results = evaluate_qwen_no_retrieval(\n",
        "    test_df,\n",
        "    n_samples=5,\n",
        "    temperature=0.3,\n",
        "    max_items=100,\n",
        "    std_threshold=30.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiM-WO_OtZqQ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# 8. Plots\n",
        "# ============================================================\n",
        "\n",
        "def plot_scatter(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    errors = np.abs(y_true - y_pred)\n",
        "    sc = plt.scatter(\n",
        "        y_true, y_pred,\n",
        "        c=errors,\n",
        "        cmap=\"viridis\",\n",
        "        alpha=0.5,\n",
        "        s=12,\n",
        "        edgecolors=\"none\",\n",
        "    )\n",
        "    max_v = max(y_true.max(), y_pred.max())\n",
        "    plt.plot([0, max_v], [0, max_v], \"r--\", label=\"y = x\")\n",
        "    plt.xlabel(\"True price (USD)\")\n",
        "    plt.ylabel(\"Predicted price (USD)\")\n",
        "    plt.title(title)\n",
        "    cbar = plt.colorbar(sc)\n",
        "    cbar.set_label(\"Absolute error\")\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_error_hist(y_true, y_pred, title):\n",
        "    errors = y_pred - y_true\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(errors, bins=40, alpha=0.7, color=\"steelblue\")\n",
        "    plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"No error\")\n",
        "    plt.xlabel(\"Prediction error (pred - true)\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(title + \" – error distribution\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ---- Call plots for key models ----\n",
        "\n",
        "# KNN baseline\n",
        "plot_scatter(knn_results[\"y_true\"], knn_results[\"y_pred\"], \"KNN baseline\")\n",
        "plot_error_hist(knn_results[\"y_true\"], knn_results[\"y_pred\"], \"KNN baseline\")\n",
        "\n",
        "# Best tree model you have (example: Random Forest; switch to GBM if better)\n",
        "plot_scatter(rf_results[\"y_true\"], rf_results[\"y_pred\"], \"RandomForest baseline\")\n",
        "plot_error_hist(rf_results[\"y_true\"], rf_results[\"y_pred\"], \"RandomForest baseline\")\n",
        "\n",
        "# Qwen + retrieval (main method), if available\n",
        "if qwen_ret_results is not None:\n",
        "    plot_scatter(qwen_ret_results[\"y_true\"], qwen_ret_results[\"y_pred\"], \"Qwen + retrieval\")\n",
        "    plot_error_hist(qwen_ret_results[\"y_true\"], qwen_ret_results[\"y_pred\"], \"Qwen + retrieval\")\n",
        "\n",
        "# Qwen no-retrieval baseline, if available\n",
        "if qwen_no_ret_results is not None:\n",
        "    plot_scatter(qwen_no_ret_results[\"y_true\"], qwen_no_ret_results[\"y_pred\"], \"Qwen NO retrieval\")\n",
        "    plot_error_hist(qwen_no_ret_results[\"y_true\"], qwen_no_ret_results[\"y_pred\"], \"Qwen NO retrieval\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}